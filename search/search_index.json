{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"learningOrchestra: a distributed machine learning processing tool Objective The goal of this work is to develop a tool, named learningOrchestra , to facilitate and streamline the data science iterative process of: Gathering data Cleaning/preparing the datasets Building models Validating their predictions, and Deploying the results Architecture The architecture of learningOrchestra is a collection of microservices deployed in a cluster. A dataset (in CSV format) can be loaded from an URL using the Database API microservice, which converts the dataset to JSON and later stores it in MongoDB. It is also possible to perform several preprocessing and analytical tasks using learningOrchestra's collection of microservices . With learningOrchestra, you can build prediction models with different classifiers simultaneously using stored and preprocessed datasets with the Model Builder microservice. This microservice uses a Spark cluster to make prediction models using distributed processing. You can compare the different classification results over time to fit and increase prediction accuracy. By providing their own preprocessing code, users can create highly customized model predictions against a specific dataset, increasing model prediction accuracy. With that in mind, the possibilities are endless! \ud83d\ude80 Getting Started To make using learningOrchestra more accessible, we provide the learning_orchestra_client Python package. This package provides developers with all of learningOrchestra's functionalities in a Python API. To improve user experience, a user can export and analyse the results using a MongoDB GUI framework, such as NoSQLBooster . We also built a demo of learningOrchestra (in learning_orchestra_client usage example section) with the Titanic challenge dataset . This documentation has a more detailed description on how to install and use it. We also provide documentation and examples for each microservice and the Python package.","title":"Whats is?"},{"location":"#learningorchestra-a-distributed-machine-learning-processing-tool","text":"","title":"learningOrchestra: a distributed machine learning processing tool"},{"location":"#objective","text":"The goal of this work is to develop a tool, named learningOrchestra , to facilitate and streamline the data science iterative process of: Gathering data Cleaning/preparing the datasets Building models Validating their predictions, and Deploying the results","title":"Objective"},{"location":"#architecture","text":"The architecture of learningOrchestra is a collection of microservices deployed in a cluster. A dataset (in CSV format) can be loaded from an URL using the Database API microservice, which converts the dataset to JSON and later stores it in MongoDB. It is also possible to perform several preprocessing and analytical tasks using learningOrchestra's collection of microservices . With learningOrchestra, you can build prediction models with different classifiers simultaneously using stored and preprocessed datasets with the Model Builder microservice. This microservice uses a Spark cluster to make prediction models using distributed processing. You can compare the different classification results over time to fit and increase prediction accuracy. By providing their own preprocessing code, users can create highly customized model predictions against a specific dataset, increasing model prediction accuracy. With that in mind, the possibilities are endless! \ud83d\ude80","title":"Architecture"},{"location":"#getting-started","text":"To make using learningOrchestra more accessible, we provide the learning_orchestra_client Python package. This package provides developers with all of learningOrchestra's functionalities in a Python API. To improve user experience, a user can export and analyse the results using a MongoDB GUI framework, such as NoSQLBooster . We also built a demo of learningOrchestra (in learning_orchestra_client usage example section) with the Titanic challenge dataset . This documentation has a more detailed description on how to install and use it. We also provide documentation and examples for each microservice and the Python package.","title":"Getting Started"},{"location":"data_type_handler/","text":"Data Type Handler Microservice This microservice changes data type from stored file between number and string. Change fields type of inserted file PATCH CLUSTER_IP:5003/fieldtypes/<filename> The request uses filename as the id in the parameters and fields in the body, fields is an array with all fields from file to be changed, using number or string descriptor in each Key:Value to describe the new value of altered field of file. { \"field1\" : \"number\" , \"field2\" : \"string\" }","title":"Data Type Handler"},{"location":"data_type_handler/#data-type-handler-microservice","text":"This microservice changes data type from stored file between number and string.","title":"Data Type Handler Microservice"},{"location":"data_type_handler/#change-fields-type-of-inserted-file","text":"PATCH CLUSTER_IP:5003/fieldtypes/<filename> The request uses filename as the id in the parameters and fields in the body, fields is an array with all fields from file to be changed, using number or string descriptor in each Key:Value to describe the new value of altered field of file. { \"field1\" : \"number\" , \"field2\" : \"string\" }","title":"Change fields type of inserted file"},{"location":"database_api/","text":"Database API microservice The Database API microservice creates a level of abstraction through a REST API. Using MongoDB, datasets are downloaded in CSV format and parsed into JSON format where the primary key for each document is the filename field contained in the JSON file POST request. GUI tool to handle database files There are GUI tools to handle database files, for example, NoSQLBooster can interact with mongoDB used in database, and makes several tasks which are limited in learning\\_orchestra\\_client package , as schema visualization and files extraction and download to formats as CSV, JSON, you also can navigate in all inserted files in easy way and visualize each row from determined file, to use this tool connect with the url cluster\\_ip:27017 and use the credentials username = root password = owl45#21 List all inserted files GET CLUSTER_IP:5000/files Returns an array of metadata files from the database, where each file contains a metadata file. Downloaded files metadata { \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Ticket\" , \"Fare\" , \"Cabin\" , \"Embarked\" ], \"filename\" : \"titanic_training\" , \"finished\" : true , \"time_created\" : \"2020-07-28T22:16:10-00:00\" , \"url\" : \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file filename - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished time_created - Time of creation url - URL used to download the file Preprocessed files metadata { \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Embarked\" ], \"filename\" : \"titanic_training_projection\" , \"finished\" : false , \"parent_filename\" : \"titanic_training\" , \"time_created\" : \"2020-07-28T12:01:44-00:00\" } parent_filename - The filename used to make a preprocess task, from which the current file is derived. Classifier prediction files metadata { \"F1\" : \"0.7030995388400528\" , \"accuracy\" : \"0.7034883720930233\" , \"classificator\" : \"nb\" , \"filename\" : \"titanic_testing_new_prediction_nb\" , \"fit_time\" : 41.870062828063965 } F1 - F1 Score from model accuracy accuracy - Accuracy rate from model prediction classificator - Initials from used classificator filename - Name of the file fit_time - Time taken for the model to be fit during training List file content GET CLUSTER_IP:5000/files/<filename>?skip=number&limit=number&query={} Returns rows of the file requested, with pagination filename - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file. Post file POST CLUSTER_IP:5000/files Insert a CSV into the database using the POST method, JSON must be contained in the body of the HTTP request. The following fields are required: { \"filename\" : \"key_to_document_identification\" , \"url\" : \"http://sitetojson.file/path/to/csv\" } Delete an existing file DELETE CLUSTER_IP:5000/files/<filename> Request of type DELETE , passing the filename field of an existing file in the request parameters, deleting the file in the database.","title":"Database API"},{"location":"database_api/#database-api-microservice","text":"The Database API microservice creates a level of abstraction through a REST API. Using MongoDB, datasets are downloaded in CSV format and parsed into JSON format where the primary key for each document is the filename field contained in the JSON file POST request.","title":"Database API microservice"},{"location":"database_api/#gui-tool-to-handle-database-files","text":"There are GUI tools to handle database files, for example, NoSQLBooster can interact with mongoDB used in database, and makes several tasks which are limited in learning\\_orchestra\\_client package , as schema visualization and files extraction and download to formats as CSV, JSON, you also can navigate in all inserted files in easy way and visualize each row from determined file, to use this tool connect with the url cluster\\_ip:27017 and use the credentials username = root password = owl45#21","title":"GUI tool to handle database files"},{"location":"database_api/#list-all-inserted-files","text":"GET CLUSTER_IP:5000/files Returns an array of metadata files from the database, where each file contains a metadata file.","title":"List all inserted files"},{"location":"database_api/#downloaded-files-metadata","text":"{ \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Ticket\" , \"Fare\" , \"Cabin\" , \"Embarked\" ], \"filename\" : \"titanic_training\" , \"finished\" : true , \"time_created\" : \"2020-07-28T22:16:10-00:00\" , \"url\" : \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file filename - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished time_created - Time of creation url - URL used to download the file","title":"Downloaded files metadata"},{"location":"database_api/#preprocessed-files-metadata","text":"{ \"fields\" : [ \"PassengerId\" , \"Survived\" , \"Pclass\" , \"Name\" , \"Sex\" , \"Age\" , \"SibSp\" , \"Parch\" , \"Embarked\" ], \"filename\" : \"titanic_training_projection\" , \"finished\" : false , \"parent_filename\" : \"titanic_training\" , \"time_created\" : \"2020-07-28T12:01:44-00:00\" } parent_filename - The filename used to make a preprocess task, from which the current file is derived.","title":"Preprocessed files metadata"},{"location":"database_api/#classifier-prediction-files-metadata","text":"{ \"F1\" : \"0.7030995388400528\" , \"accuracy\" : \"0.7034883720930233\" , \"classificator\" : \"nb\" , \"filename\" : \"titanic_testing_new_prediction_nb\" , \"fit_time\" : 41.870062828063965 } F1 - F1 Score from model accuracy accuracy - Accuracy rate from model prediction classificator - Initials from used classificator filename - Name of the file fit_time - Time taken for the model to be fit during training","title":"Classifier prediction files metadata"},{"location":"database_api/#list-file-content","text":"GET CLUSTER_IP:5000/files/<filename>?skip=number&limit=number&query={} Returns rows of the file requested, with pagination filename - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file.","title":"List file content"},{"location":"database_api/#post-file","text":"POST CLUSTER_IP:5000/files Insert a CSV into the database using the POST method, JSON must be contained in the body of the HTTP request. The following fields are required: { \"filename\" : \"key_to_document_identification\" , \"url\" : \"http://sitetojson.file/path/to/csv\" }","title":"Post file"},{"location":"database_api/#delete-an-existing-file","text":"DELETE CLUSTER_IP:5000/files/<filename> Request of type DELETE , passing the filename field of an existing file in the request parameters, deleting the file in the database.","title":"Delete an existing file"},{"location":"histogram/","text":"Histogram microservice Microservice used to make the histogram from a stored file, storing the resulting histogram in a new file in MongoDB. Create a Histogram from posted file POST CLUSTER_IP:5004/histograms/<filename> The request is sent in the body, histogram_filename is the name of the file in which the histogram result is saved to and fields is an array with all the fields necessary to make the histogram. { \"histogram_filename\" : \"filename_to_save_the_histogram\" , \"fields\" : [ \"fields\" , \"from\" , \"filename\" ] }","title":"Histogram"},{"location":"histogram/#histogram-microservice","text":"Microservice used to make the histogram from a stored file, storing the resulting histogram in a new file in MongoDB.","title":"Histogram microservice"},{"location":"histogram/#create-a-histogram-from-posted-file","text":"POST CLUSTER_IP:5004/histograms/<filename> The request is sent in the body, histogram_filename is the name of the file in which the histogram result is saved to and fields is an array with all the fields necessary to make the histogram. { \"histogram_filename\" : \"filename_to_save_the_histogram\" , \"fields\" : [ \"fields\" , \"from\" , \"filename\" ] }","title":"Create a Histogram from posted file"},{"location":"install/","text":"Install Requirements Linux hosts; Docker Engine installed in all instances of your cluster; Cluster configured in swarm mode, more details in swarm documentation ; Docker Compose installed in manager instance of your cluster; and Ensure which your cluster environment has no network traffic block, as firewalls rules in your network or owner firewall in linux hosts, case has firewalls or other blockers, insert learningOrchestra in blocked exceptions, as example, in Google Cloud Platform the VMs must be with allow_http and allow_https firewall rules allowed in each VM configuration. Deploy In your manager docker swarm machine, ensure that you are located in the project root ./learningOrchestra and run the command below in sudo mode to deploy the learningOrchestra: sudo ./run.sh If everything goes according to plan, learningOrchestra has been deployed in your swarm cluster. Congrulations! \ud83e\udd73 \ud83d\udc4f learningOrchestra Cluster State There are two web pages for cluster state visualization: Visualize cluster state (deployed microservices and cluster's machines) - CLUSTER_IP:80 Visualize spark cluster state - CLUSTER_IP:8080 . The CLUSTER_IP is an external IP of a machine in your cluster.","title":"Install"},{"location":"install/#install","text":"","title":"Install"},{"location":"install/#requirements","text":"Linux hosts; Docker Engine installed in all instances of your cluster; Cluster configured in swarm mode, more details in swarm documentation ; Docker Compose installed in manager instance of your cluster; and Ensure which your cluster environment has no network traffic block, as firewalls rules in your network or owner firewall in linux hosts, case has firewalls or other blockers, insert learningOrchestra in blocked exceptions, as example, in Google Cloud Platform the VMs must be with allow_http and allow_https firewall rules allowed in each VM configuration.","title":"Requirements"},{"location":"install/#deploy","text":"In your manager docker swarm machine, ensure that you are located in the project root ./learningOrchestra and run the command below in sudo mode to deploy the learningOrchestra: sudo ./run.sh If everything goes according to plan, learningOrchestra has been deployed in your swarm cluster. Congrulations! \ud83e\udd73 \ud83d\udc4f","title":"Deploy"},{"location":"install/#learningorchestra-cluster-state","text":"There are two web pages for cluster state visualization: Visualize cluster state (deployed microservices and cluster's machines) - CLUSTER_IP:80 Visualize spark cluster state - CLUSTER_IP:8080 . The CLUSTER_IP is an external IP of a machine in your cluster.","title":"learningOrchestra Cluster State"},{"location":"learning_orchestra_client_package/","text":"learningOrchestra Client Package You can know how install and use this package at learning_orchestra_client repository.","title":"Python Package"},{"location":"learning_orchestra_client_package/#learningorchestra-client-package","text":"You can know how install and use this package at learning_orchestra_client repository.","title":"learningOrchestra Client Package"},{"location":"model_builder/","text":"Model Builder Microservice Model Builder microservice provides a REST API to create several model predictions using your own preprocessing code using a defined set of classifiers. Create prediction model POST CLUSTER_IP:5002/models { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : \"String list of classificators to be used\" } List of Classifiers lr : LogisticRegression dt : DecisionTreeClassifier rf : RandomForestClassifier gb : Gradient-boosted tree classifier nb : NaiveBayes To send a request with LogisticRegression and NaiveBayes Classifiers: { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : [ \"lr\" , \"nb\" ] } preprocessor_code environment The python 3 preprocessing code must use the environment instances in bellow: training_df (Instantiated): Spark Dataframe instance training filename testing_df (Instantiated): Spark Dataframe instance testing filename The preprocessing code must instantiate the variables in below, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for train the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluating trained model accuracy features_testing (Not Instantiated): Spark Dataframe instance for testing the model In case you don't want to evaluate the model, set features_evaluation as None . Handy methods self . fields_from_dataframe ( self , dataframe , is_string ) This method returns string or number fields as a string list from a DataFrame. dataframe : DataFrame instance is_string : Boolean parameter, if True , the method returns the string DataFrame fields, otherwise, returns the numbers DataFrame fields. preprocessor_code Example This example uses the titanic challengue datasets . from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean , col , split , regexp_extract , when , lit ) from pyspark.ml.feature import ( VectorAssembler , StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df . withColumnRenamed ( 'Survived' , 'label' ) testing_df = testing_df . withColumn ( 'label' , lit ( 0 )) datasets_list = [ training_df , testing_df ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Initial\" , regexp_extract ( col ( \"Name\" ), \"([A-Za-z]+)\\.\" , 1 )) datasets_list [ index ] = dataset misspelled_initials = [ 'Mlle' , 'Mme' , 'Ms' , 'Dr' , 'Major' , 'Lady' , 'Countess' , 'Jonkheer' , 'Col' , 'Rev' , 'Capt' , 'Sir' , 'Don' ] correct_initials = [ 'Miss' , 'Miss' , 'Miss' , 'Mr' , 'Mr' , 'Mrs' , 'Mrs' , 'Other' , 'Other' , 'Other' , 'Mr' , 'Mr' , 'Mr' ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . replace ( misspelled_initials , correct_initials ) datasets_list [ index ] = dataset initials_age = { \"Miss\" : 22 , \"Other\" : 46 , \"Master\" : 5 , \"Mr\" : 33 , \"Mrs\" : 36 } for index , dataset in enumerate ( datasets_list ): for initial , initial_age in initials_age . items (): dataset = dataset . withColumn ( \"Age\" , when (( dataset [ \"Initial\" ] == initial ) & ( dataset [ \"Age\" ] . isNull ()), initial_age ) . otherwise ( dataset [ \"Age\" ])) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . na . fill ({ \"Embarked\" : 'S' }) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Family_Size\" , col ( 'SibSp' ) + col ( 'Parch' )) dataset = dataset . withColumn ( 'Alone' , lit ( 0 )) dataset = dataset . withColumn ( \"Alone\" , when ( dataset [ \"Family_Size\" ] == 0 , 1 ) . otherwise ( dataset [ \"Alone\" ])) datasets_list [ index ] = dataset text_fields = [ \"Sex\" , \"Embarked\" , \"Initial\" ] for column in text_fields : for index , dataset in enumerate ( datasets_list ): dataset = StringIndexer ( inputCol = column , outputCol = column + \"_index\" ) . \\ fit ( dataset ) . \\ transform ( dataset ) datasets_list [ index ] = dataset non_required_columns = [ \"Name\" , \"Embarked\" , \"Sex\" , \"Initial\" ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . drop ( * non_required_columns ) datasets_list [ index ] = dataset training_df = datasets_list [ TRAINING_DF_INDEX ] testing_df = datasets_list [ TESTING_DF_INDEX ] assembler = VectorAssembler ( inputCols = training_df . columns [:], outputCol = \"features\" ) assembler . setHandleInvalid ( 'skip' ) features_training = assembler . transform ( training_df ) ( features_training , features_evaluation ) = \\ features_training . randomSplit ([ 0.8 , 0.2 ], seed = 33 ) features_testing = assembler . transform ( testing_df )","title":"Model Builder"},{"location":"model_builder/#model-builder-microservice","text":"Model Builder microservice provides a REST API to create several model predictions using your own preprocessing code using a defined set of classifiers.","title":"Model Builder Microservice"},{"location":"model_builder/#create-prediction-model","text":"POST CLUSTER_IP:5002/models { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : \"String list of classificators to be used\" }","title":"Create prediction model"},{"location":"model_builder/#list-of-classifiers","text":"lr : LogisticRegression dt : DecisionTreeClassifier rf : RandomForestClassifier gb : Gradient-boosted tree classifier nb : NaiveBayes To send a request with LogisticRegression and NaiveBayes Classifiers: { \"training_filename\" : \"training filename\" , \"test_filename\" : \"test filename\" , \"preprocessor_code\" : \"Python3 code to preprocessing, using Pyspark library\" , \"classificators_list\" : [ \"lr\" , \"nb\" ] }","title":"List of Classifiers"},{"location":"model_builder/#preprocessor_code-environment","text":"The python 3 preprocessing code must use the environment instances in bellow: training_df (Instantiated): Spark Dataframe instance training filename testing_df (Instantiated): Spark Dataframe instance testing filename The preprocessing code must instantiate the variables in below, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for train the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluating trained model accuracy features_testing (Not Instantiated): Spark Dataframe instance for testing the model In case you don't want to evaluate the model, set features_evaluation as None .","title":"preprocessor_code environment"},{"location":"model_builder/#handy-methods","text":"self . fields_from_dataframe ( self , dataframe , is_string ) This method returns string or number fields as a string list from a DataFrame. dataframe : DataFrame instance is_string : Boolean parameter, if True , the method returns the string DataFrame fields, otherwise, returns the numbers DataFrame fields.","title":"Handy methods"},{"location":"model_builder/#preprocessor_code-example","text":"This example uses the titanic challengue datasets . from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean , col , split , regexp_extract , when , lit ) from pyspark.ml.feature import ( VectorAssembler , StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df . withColumnRenamed ( 'Survived' , 'label' ) testing_df = testing_df . withColumn ( 'label' , lit ( 0 )) datasets_list = [ training_df , testing_df ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Initial\" , regexp_extract ( col ( \"Name\" ), \"([A-Za-z]+)\\.\" , 1 )) datasets_list [ index ] = dataset misspelled_initials = [ 'Mlle' , 'Mme' , 'Ms' , 'Dr' , 'Major' , 'Lady' , 'Countess' , 'Jonkheer' , 'Col' , 'Rev' , 'Capt' , 'Sir' , 'Don' ] correct_initials = [ 'Miss' , 'Miss' , 'Miss' , 'Mr' , 'Mr' , 'Mrs' , 'Mrs' , 'Other' , 'Other' , 'Other' , 'Mr' , 'Mr' , 'Mr' ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . replace ( misspelled_initials , correct_initials ) datasets_list [ index ] = dataset initials_age = { \"Miss\" : 22 , \"Other\" : 46 , \"Master\" : 5 , \"Mr\" : 33 , \"Mrs\" : 36 } for index , dataset in enumerate ( datasets_list ): for initial , initial_age in initials_age . items (): dataset = dataset . withColumn ( \"Age\" , when (( dataset [ \"Initial\" ] == initial ) & ( dataset [ \"Age\" ] . isNull ()), initial_age ) . otherwise ( dataset [ \"Age\" ])) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . na . fill ({ \"Embarked\" : 'S' }) datasets_list [ index ] = dataset for index , dataset in enumerate ( datasets_list ): dataset = dataset . withColumn ( \"Family_Size\" , col ( 'SibSp' ) + col ( 'Parch' )) dataset = dataset . withColumn ( 'Alone' , lit ( 0 )) dataset = dataset . withColumn ( \"Alone\" , when ( dataset [ \"Family_Size\" ] == 0 , 1 ) . otherwise ( dataset [ \"Alone\" ])) datasets_list [ index ] = dataset text_fields = [ \"Sex\" , \"Embarked\" , \"Initial\" ] for column in text_fields : for index , dataset in enumerate ( datasets_list ): dataset = StringIndexer ( inputCol = column , outputCol = column + \"_index\" ) . \\ fit ( dataset ) . \\ transform ( dataset ) datasets_list [ index ] = dataset non_required_columns = [ \"Name\" , \"Embarked\" , \"Sex\" , \"Initial\" ] for index , dataset in enumerate ( datasets_list ): dataset = dataset . drop ( * non_required_columns ) datasets_list [ index ] = dataset training_df = datasets_list [ TRAINING_DF_INDEX ] testing_df = datasets_list [ TESTING_DF_INDEX ] assembler = VectorAssembler ( inputCols = training_df . columns [:], outputCol = \"features\" ) assembler . setHandleInvalid ( 'skip' ) features_training = assembler . transform ( training_df ) ( features_training , features_evaluation ) = \\ features_training . randomSplit ([ 0.8 , 0.2 ], seed = 33 ) features_testing = assembler . transform ( testing_df )","title":"preprocessor_code Example"},{"location":"pca/","text":"PCA microservice PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn (used in this microservice), PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components. PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter whiten = True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm, more information about this algorithm in scikit-learn PCA docs . Create an image plot POST CLUSTER_IP:5006/images/<parent_filename> The request uses a parent_filename as a dataset filename, the body contains the json fields: { \"pca_filename\" : \"image_plot_filename\" , \"label_name\" : \"dataset_label_column\" } The label_name is the label name column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in JSON: { \"pca_filename\" : \"image_plot_filename\" , \"label_name\" : null } Delete an image plot DELETE CLUSTER_IP:5006/images/<image_plot_filename> Deletes an image plot from the database. Read the filenames of the created images GET CLUSTER_IP:5006/images Returns a list with all created image plot filenames. Read an image plot GET CLUSTER_IP:5006/images/<image_plot_filename> Returns the image plot of filename specified. Images plot examples This examples use the titanic challengue datasets . Titanic Train dataset Titanic Test dataset","title":"PCA"},{"location":"pca/#pca-microservice","text":"PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn (used in this microservice), PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components. PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter whiten = True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm, more information about this algorithm in scikit-learn PCA docs .","title":"PCA microservice"},{"location":"pca/#create-an-image-plot","text":"POST CLUSTER_IP:5006/images/<parent_filename> The request uses a parent_filename as a dataset filename, the body contains the json fields: { \"pca_filename\" : \"image_plot_filename\" , \"label_name\" : \"dataset_label_column\" } The label_name is the label name column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in JSON: { \"pca_filename\" : \"image_plot_filename\" , \"label_name\" : null }","title":"Create an image plot"},{"location":"pca/#delete-an-image-plot","text":"DELETE CLUSTER_IP:5006/images/<image_plot_filename> Deletes an image plot from the database.","title":"Delete an image plot"},{"location":"pca/#read-the-filenames-of-the-created-images","text":"GET CLUSTER_IP:5006/images Returns a list with all created image plot filenames.","title":"Read the filenames of the created images"},{"location":"pca/#read-an-image-plot","text":"GET CLUSTER_IP:5006/images/<image_plot_filename> Returns the image plot of filename specified.","title":"Read an image plot"},{"location":"pca/#images-plot-examples","text":"This examples use the titanic challengue datasets .","title":"Images plot examples"},{"location":"pca/#titanic-train-dataset","text":"","title":"Titanic Train dataset"},{"location":"pca/#titanic-test-dataset","text":"","title":"Titanic Test dataset"},{"location":"projection/","text":"Projection Microservice Projection microservice provides an api to make a projection from file inserted in database service, generating a new file and putting in database. Create projection from a inserted file POST CLUSTER_IP:5001/projections/<filename> Post request where filename is the name of the file to create a projection for. { \"projection_filename\" : \"filename_to_save_projection\" , \"fields\" : [ \"list\" , \"of\" , \"fields\" ] }","title":"Projection"},{"location":"projection/#projection-microservice","text":"Projection microservice provides an api to make a projection from file inserted in database service, generating a new file and putting in database.","title":"Projection Microservice"},{"location":"projection/#create-projection-from-a-inserted-file","text":"POST CLUSTER_IP:5001/projections/<filename> Post request where filename is the name of the file to create a projection for. { \"projection_filename\" : \"filename_to_save_projection\" , \"fields\" : [ \"list\" , \"of\" , \"fields\" ] }","title":"Create projection from a inserted file"},{"location":"t_sne/","text":"t-SNE Microservice The T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability, more information about this algorithm in its Wiki page . Create an image plot POST CLUSTER_IP:5005/images/<parent_filename> The request uses a parent_filename as a dataset inserted filename, the body contains the JSON fields: { \"tsne_filename\" : \"image_plot_filename\" , \"label_name\" : \"dataset_label_column\" } The label_name is the label name of the column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"tsne_filename\" : \"image_plot_filename\" , \"label_name\" : null } Delete an image plot DELETE CLUSTER_IP:5005/images/<image_plot_filename> Deletes an image plot by specifying its file name. Read the filenames of the created images GET CLUSTER_IP:5005/images Returns a list with all created images plot file name. Read an image plot GET CLUSTER_IP:5005/images/<image_plot_filename> Returns the image plot of the specified file name. Images plot examples This examples use the titanic challengue datasets . Titanic Train dataset Titanic Test dataset","title":"t-SNE"},{"location":"t_sne/#t-sne-microservice","text":"The T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability, more information about this algorithm in its Wiki page .","title":"t-SNE Microservice"},{"location":"t_sne/#create-an-image-plot","text":"POST CLUSTER_IP:5005/images/<parent_filename> The request uses a parent_filename as a dataset inserted filename, the body contains the JSON fields: { \"tsne_filename\" : \"image_plot_filename\" , \"label_name\" : \"dataset_label_column\" } The label_name is the label name of the column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"tsne_filename\" : \"image_plot_filename\" , \"label_name\" : null }","title":"Create an image plot"},{"location":"t_sne/#delete-an-image-plot","text":"DELETE CLUSTER_IP:5005/images/<image_plot_filename> Deletes an image plot by specifying its file name.","title":"Delete an image plot"},{"location":"t_sne/#read-the-filenames-of-the-created-images","text":"GET CLUSTER_IP:5005/images Returns a list with all created images plot file name.","title":"Read the filenames of the created images"},{"location":"t_sne/#read-an-image-plot","text":"GET CLUSTER_IP:5005/images/<image_plot_filename> Returns the image plot of the specified file name.","title":"Read an image plot"},{"location":"t_sne/#images-plot-examples","text":"This examples use the titanic challengue datasets .","title":"Images plot examples"},{"location":"t_sne/#titanic-train-dataset","text":"","title":"Titanic Train dataset"},{"location":"t_sne/#titanic-test-dataset","text":"","title":"Titanic Test dataset"},{"location":"usage/","text":"Usage Python package learningOrchestra Client - The python package for learningOrchestra use Microservices REST API database api - Microservice used to download and handling files in database; projection - Makes projections of stored files in database using spark cluster; data type handler - Changes fields file type between number and text; histogram - Makes histograms of stored files in database; t-SNE - Makes a t-SNE image plot of stored files in database; PCA - Makes a PCA image plot of stored files in database; and model builder - Creates a prediction model from preprocessed files using spark cluster. Spark microservice The projection, t-SNE, PCA and model builder microservices use the spark microservice to make your work. By default, this microservice has only one instance. In case your data processing requires more computing processing power, you can scale this microservice to earn computing power. To do this, with learningOrchestra already deployed, in the master machine of your docker swarm cluster, run: docker service scale microservice_sparkworker=NUMBER_OF_INSTANCES The NUMBER_OF_INSTANCES is the amount of spark microservice instances which you desire to be created in your cluster, this number must be choosen according to your cluster resources and the resource requirements of your task. Database GUI NoSQLBooster - MongoDB GUI performs several database tasks, such as files visualization, queries, projections and files extraction to formats as CSV and JSON, read the database api docs to learn how to configure this tool.","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#python-package","text":"learningOrchestra Client - The python package for learningOrchestra use","title":"Python package"},{"location":"usage/#microservices-rest-api","text":"database api - Microservice used to download and handling files in database; projection - Makes projections of stored files in database using spark cluster; data type handler - Changes fields file type between number and text; histogram - Makes histograms of stored files in database; t-SNE - Makes a t-SNE image plot of stored files in database; PCA - Makes a PCA image plot of stored files in database; and model builder - Creates a prediction model from preprocessed files using spark cluster.","title":"Microservices REST API"},{"location":"usage/#spark-microservice","text":"The projection, t-SNE, PCA and model builder microservices use the spark microservice to make your work. By default, this microservice has only one instance. In case your data processing requires more computing processing power, you can scale this microservice to earn computing power. To do this, with learningOrchestra already deployed, in the master machine of your docker swarm cluster, run: docker service scale microservice_sparkworker=NUMBER_OF_INSTANCES The NUMBER_OF_INSTANCES is the amount of spark microservice instances which you desire to be created in your cluster, this number must be choosen according to your cluster resources and the resource requirements of your task.","title":"Spark microservice"},{"location":"usage/#database-gui","text":"NoSQLBooster - MongoDB GUI performs several database tasks, such as files visualization, queries, projections and files extraction to formats as CSV and JSON, read the database api docs to learn how to configure this tool.","title":"Database GUI"}]}