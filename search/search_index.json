{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"learningOrchestra Docs learningOrchestra is a distributed processing tool that facilitates and streamlines iterative processes in a Data Science project pipeline like: Data Gathering Data Cleaning Model Building Validating the Model Presenting the Results With learningOrchestra, you can: load a dataset from an URL (in CSV format). accomplish several pre-processing tasks with datasets. create highly customised model predictions against a specific dataset by providing their own pre-processing code. build prediction models with different classifiers simultaneously using a spark cluster transparently. And so much more! Check the Usage section for more.","title":"Home"},{"location":"#learningorchestra-docs","text":"learningOrchestra is a distributed processing tool that facilitates and streamlines iterative processes in a Data Science project pipeline like: Data Gathering Data Cleaning Model Building Validating the Model Presenting the Results With learningOrchestra, you can: load a dataset from an URL (in CSV format). accomplish several pre-processing tasks with datasets. create highly customised model predictions against a specific dataset by providing their own pre-processing code. build prediction models with different classifiers simultaneously using a spark cluster transparently. And so much more! Check the Usage section for more.","title":"learningOrchestra Docs"},{"location":"database-api/","text":"Database API The Database API microservice creates a level of abstraction through a REST API. Using MongoDB, datasets are downloaded in CSV format and parsed into JSON format where the primary key for each document is the filename field contained in the JSON file POST request. GUI tool to handle database files There are GUI tools to handle database files, like NoSQLBooster can interact with mongoDB used in database, and makes several tasks which are limited in learning-orchestra-client package, as schema visualization and files extraction and download to formats as CSV and JSON. You also can navigate in all inserted files in easy way and visualize each row from determined file, to use this tool connect with the url cluster\\_ip:27017 and use the credentials: username = root password = owl45#21 List all inserted files GET CLUSTER_IP:5000/files Returns an array of metadata files from the database, where each file contains a metadata file. Downloaded files Metadata { \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\" ], \"filename\": \"titanic_training\", \"finished\": true, \"time_created\": \"2020-07-28T22:16:10-00:00\", \"url\": \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file filename - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished time_created - Time of creation url - URL used to download the file Preprocessed files metadata { \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\" ], \"filename\": \"titanic_training_projection\", \"finished\": false, \"parent_filename\": \"titanic_training\", \"time_created\": \"2020-07-28T12:01:44-00:00\" } parent_filename - The filename used to make a preprocess task, from which the current file is derived. Classifier prediction files metadata { \"F1\": \"0.7030995388400528\", \"accuracy\": \"0.7034883720930233\", \"classificator\": \"nb\", \"filename\": \"titanic_testing_new_prediction_nb\", \"fit_time\": 41.870062828063965 } F1 - F1 Score from model accuracy accuracy - Accuracy from model prediction classificator - Initials from used classificator filename - Name of the file fit_time - Time taken for the model to be fit during training List file content GET CLUSTER_IP:5000/files/<filename>?skip=number&limit=number&query={} Returns rows of the file requested, with pagination. filename - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file. Post file POST CLUSTER_IP:5000/files Insert a CSV into the database using the POST method, JSON must be contained in the body of the HTTP request. The following fields are required: { \"filename\": \"key_to_document_identification\", \"url\": \"http://sitetojson.file/path/to/csv\" } Delete an existing file DELETE CLUSTER_IP:5000/files/<filename> Request of type DELETE , passing the filename field of an existing file in the request parameters, deleting the file in the database.","title":"Database API"},{"location":"database-api/#database-api","text":"The Database API microservice creates a level of abstraction through a REST API. Using MongoDB, datasets are downloaded in CSV format and parsed into JSON format where the primary key for each document is the filename field contained in the JSON file POST request.","title":"Database API"},{"location":"database-api/#gui-tool-to-handle-database-files","text":"There are GUI tools to handle database files, like NoSQLBooster can interact with mongoDB used in database, and makes several tasks which are limited in learning-orchestra-client package, as schema visualization and files extraction and download to formats as CSV and JSON. You also can navigate in all inserted files in easy way and visualize each row from determined file, to use this tool connect with the url cluster\\_ip:27017 and use the credentials: username = root password = owl45#21","title":"GUI tool to handle database files"},{"location":"database-api/#list-all-inserted-files","text":"GET CLUSTER_IP:5000/files Returns an array of metadata files from the database, where each file contains a metadata file.","title":"List all inserted files"},{"location":"database-api/#downloaded-files-metadata","text":"{ \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\" ], \"filename\": \"titanic_training\", \"finished\": true, \"time_created\": \"2020-07-28T22:16:10-00:00\", \"url\": \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file filename - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished time_created - Time of creation url - URL used to download the file","title":"Downloaded files Metadata"},{"location":"database-api/#preprocessed-files-metadata","text":"{ \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\" ], \"filename\": \"titanic_training_projection\", \"finished\": false, \"parent_filename\": \"titanic_training\", \"time_created\": \"2020-07-28T12:01:44-00:00\" } parent_filename - The filename used to make a preprocess task, from which the current file is derived.","title":"Preprocessed files metadata"},{"location":"database-api/#classifier-prediction-files-metadata","text":"{ \"F1\": \"0.7030995388400528\", \"accuracy\": \"0.7034883720930233\", \"classificator\": \"nb\", \"filename\": \"titanic_testing_new_prediction_nb\", \"fit_time\": 41.870062828063965 } F1 - F1 Score from model accuracy accuracy - Accuracy from model prediction classificator - Initials from used classificator filename - Name of the file fit_time - Time taken for the model to be fit during training","title":"Classifier prediction files metadata"},{"location":"database-api/#list-file-content","text":"GET CLUSTER_IP:5000/files/<filename>?skip=number&limit=number&query={} Returns rows of the file requested, with pagination. filename - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file.","title":"List file content"},{"location":"database-api/#post-file","text":"POST CLUSTER_IP:5000/files Insert a CSV into the database using the POST method, JSON must be contained in the body of the HTTP request. The following fields are required: { \"filename\": \"key_to_document_identification\", \"url\": \"http://sitetojson.file/path/to/csv\" }","title":"Post file"},{"location":"database-api/#delete-an-existing-file","text":"DELETE CLUSTER_IP:5000/files/<filename> Request of type DELETE , passing the filename field of an existing file in the request parameters, deleting the file in the database.","title":"Delete an existing file"},{"location":"datatype-api/","text":"Data Type API This microservice changes data type from stored file between number and string . Change field types of inserted file PATCH CLUSTER_IP:5003/fieldtypes/<filename> The request uses filename as the id in the parameters and fields in the body, fields is an array with all fields from file to be changed, using number or string descriptor in each Key:Value to describe the new value of altered field of file. { \"field1\": \"number\", \"field2\": \"string\" }","title":"Data Type API"},{"location":"datatype-api/#data-type-api","text":"This microservice changes data type from stored file between number and string .","title":"Data Type API"},{"location":"datatype-api/#change-field-types-of-inserted-file","text":"PATCH CLUSTER_IP:5003/fieldtypes/<filename> The request uses filename as the id in the parameters and fields in the body, fields is an array with all fields from file to be changed, using number or string descriptor in each Key:Value to describe the new value of altered field of file. { \"field1\": \"number\", \"field2\": \"string\" }","title":"Change field types of inserted file"},{"location":"histogram-api/","text":"Histogram API Microservice used to make a histogram from a stored file, storing the resulting histogram in a new file in MongoDB. Create a Histogram from posted file POST CLUSTER_IP:5004/histograms/<filename> The request is sent in the body, histogram_filename is the name of the file in which the histogram result is saved to and fields is an array with all the fields necessary to make the histogram. { \"histogram_filename\": \"filename_to_save_the_histogram\", \"fields\": [\"fields\", \"from\", \"filename\"] }","title":"Histogram API"},{"location":"histogram-api/#histogram-api","text":"Microservice used to make a histogram from a stored file, storing the resulting histogram in a new file in MongoDB.","title":"Histogram API"},{"location":"histogram-api/#create-a-histogram-from-posted-file","text":"POST CLUSTER_IP:5004/histograms/<filename> The request is sent in the body, histogram_filename is the name of the file in which the histogram result is saved to and fields is an array with all the fields necessary to make the histogram. { \"histogram_filename\": \"filename_to_save_the_histogram\", \"fields\": [\"fields\", \"from\", \"filename\"] }","title":"Create a Histogram from posted file"},{"location":"installation/","text":"Installation Requirements Linux hosts Docker Engine must be installed in all instances of your cluster Cluster configured in swarm mode, check creating a swarm Docker Compose must be installed in the manager instance of your cluster Ensure that your cluster environment does not block any traffic such as firewall rules in your network or in your hosts. If in case, you have firewalls or other traffic-blockers, add learningOrchestra as an exception. Ex: In Google Cloud Platform each of the VMs must allow both http and https traffic. Deployment In the manager Docker swarm machine, clone the repo using: git clone https://github.com/riibeirogabriel/learningOrchestra.git Navigate into the learningOrchestra directory and run: cd learningOrchestra sudo ./run.sh That's it! learningOrchestra has been deployed in your swarm cluster! Cluster State CLUSTER_IP:80 - To visualize cluster state (deployed microservices and cluster's machines). CLUSTER_IP:8080 - To visualize spark cluster state. * CLUSTER_IP is the external IP of a machine in your cluster.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#requirements","text":"Linux hosts Docker Engine must be installed in all instances of your cluster Cluster configured in swarm mode, check creating a swarm Docker Compose must be installed in the manager instance of your cluster Ensure that your cluster environment does not block any traffic such as firewall rules in your network or in your hosts. If in case, you have firewalls or other traffic-blockers, add learningOrchestra as an exception. Ex: In Google Cloud Platform each of the VMs must allow both http and https traffic.","title":"Requirements"},{"location":"installation/#deployment","text":"In the manager Docker swarm machine, clone the repo using: git clone https://github.com/riibeirogabriel/learningOrchestra.git Navigate into the learningOrchestra directory and run: cd learningOrchestra sudo ./run.sh That's it! learningOrchestra has been deployed in your swarm cluster!","title":"Deployment"},{"location":"installation/#cluster-state","text":"CLUSTER_IP:80 - To visualize cluster state (deployed microservices and cluster's machines). CLUSTER_IP:8080 - To visualize spark cluster state. * CLUSTER_IP is the external IP of a machine in your cluster.","title":"Cluster State"},{"location":"modelbuilder-api/","text":"Model Builder API Model Builder microservice provides a REST API to create several model predictions using your own preprocessing code using a defined set of classifiers. Create prediction model POST CLUSTER_IP:5002/models { \"training_filename\": \"training filename\", \"test_filename\": \"test filename\", \"preprocessor_code\": \"Python3 code to preprocessing, using Pyspark library\", \"classificators_list\": \"String list of classificators to be used\" } List of Classifiers lr : LogisticRegression dt : DecisionTreeClassifier rf : RandomForestClassifier gb : Gradient-boosted tree classifier nb : NaiveBayes To send a request with LogisticRegression and NaiveBayes Classifiers: { \"training_filename\": \"training filename\", \"test_filename\": \"test filename\", \"preprocessor_code\": \"Python3 code to preprocessing, using Pyspark library\", \"classificators_list\": [\"lr\", \"nb\"] } preprocessor_code environment The python 3 preprocessing code must use the environment instances in bellow: training_df (Instantiated): Spark Dataframe instance training filename testing_df (Instantiated): Spark Dataframe instance testing filename The preprocessing code must instantiate the variables in below, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for train the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluating trained model accuracy features_testing (Not Instantiated): Spark Dataframe instance for testing the model In case you don't want to evaluate the model, set features_evaluation as None . Handy methods self.fields_from_dataframe(self, dataframe, is_string) This method returns string or number fields as a string list from a DataFrame. dataframe : DataFrame instance is_string : Boolean parameter, if True , the method returns the string DataFrame fields, otherwise, returns the numbers DataFrame fields. preprocessor_code Example This example uses the titanic challengue datasets . from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean, col, split, regexp_extract, when, lit) from pyspark.ml.feature import ( VectorAssembler, StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df.withColumnRenamed('Survived', 'label') testing_df = testing_df.withColumn('label', lit(0)) datasets_list = [training_df, testing_df] for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn( \"Initial\", regexp_extract(col(\"Name\"), \"([A-Za-z]+)\\.\", 1)) datasets_list[index] = dataset misspelled_initials = [ 'Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don' ] correct_initials = [ 'Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr' ] for index, dataset in enumerate(datasets_list): dataset = dataset.replace(misspelled_initials, correct_initials) datasets_list[index] = dataset initials_age = {\"Miss\": 22, \"Other\": 46, \"Master\": 5, \"Mr\": 33, \"Mrs\": 36} for index, dataset in enumerate(datasets_list): for initial, initial_age in initials_age.items(): dataset = dataset.withColumn( \"Age\", when((dataset[\"Initial\"] == initial) & (dataset[\"Age\"].isNull()), initial_age).otherwise( dataset[\"Age\"])) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.na.fill({\"Embarked\": 'S'}) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn(\"Family_Size\", col('SibSp')+col('Parch')) dataset = dataset.withColumn('Alone', lit(0)) dataset = dataset.withColumn( \"Alone\", when(dataset[\"Family_Size\"] == 0, 1).otherwise(dataset[\"Alone\"])) datasets_list[index] = dataset text_fields = [\"Sex\", \"Embarked\", \"Initial\"] for column in text_fields: for index, dataset in enumerate(datasets_list): dataset = StringIndexer( inputCol=column, outputCol=column+\"_index\").\\ fit(dataset).\\ transform(dataset) datasets_list[index] = dataset non_required_columns = [\"Name\", \"Embarked\", \"Sex\", \"Initial\"] for index, dataset in enumerate(datasets_list): dataset = dataset.drop(*non_required_columns) datasets_list[index] = dataset training_df = datasets_list[TRAINING_DF_INDEX] testing_df = datasets_list[TESTING_DF_INDEX] assembler = VectorAssembler( inputCols=training_df.columns[:], outputCol=\"features\") assembler.setHandleInvalid('skip') features_training = assembler.transform(training_df) (features_training, features_evaluation) =\\ features_training.randomSplit([0.8, 0.2], seed=33) features_testing = assembler.transform(testing_df)","title":"Model builder API"},{"location":"modelbuilder-api/#model-builder-api","text":"Model Builder microservice provides a REST API to create several model predictions using your own preprocessing code using a defined set of classifiers.","title":"Model Builder API"},{"location":"modelbuilder-api/#create-prediction-model","text":"POST CLUSTER_IP:5002/models { \"training_filename\": \"training filename\", \"test_filename\": \"test filename\", \"preprocessor_code\": \"Python3 code to preprocessing, using Pyspark library\", \"classificators_list\": \"String list of classificators to be used\" }","title":"Create prediction model"},{"location":"modelbuilder-api/#list-of-classifiers","text":"lr : LogisticRegression dt : DecisionTreeClassifier rf : RandomForestClassifier gb : Gradient-boosted tree classifier nb : NaiveBayes To send a request with LogisticRegression and NaiveBayes Classifiers: { \"training_filename\": \"training filename\", \"test_filename\": \"test filename\", \"preprocessor_code\": \"Python3 code to preprocessing, using Pyspark library\", \"classificators_list\": [\"lr\", \"nb\"] }","title":"List of Classifiers"},{"location":"modelbuilder-api/#preprocessor_code-environment","text":"The python 3 preprocessing code must use the environment instances in bellow: training_df (Instantiated): Spark Dataframe instance training filename testing_df (Instantiated): Spark Dataframe instance testing filename The preprocessing code must instantiate the variables in below, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for train the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluating trained model accuracy features_testing (Not Instantiated): Spark Dataframe instance for testing the model In case you don't want to evaluate the model, set features_evaluation as None .","title":"preprocessor_code environment"},{"location":"modelbuilder-api/#handy-methods","text":"self.fields_from_dataframe(self, dataframe, is_string) This method returns string or number fields as a string list from a DataFrame. dataframe : DataFrame instance is_string : Boolean parameter, if True , the method returns the string DataFrame fields, otherwise, returns the numbers DataFrame fields.","title":"Handy methods"},{"location":"modelbuilder-api/#preprocessor_code-example","text":"This example uses the titanic challengue datasets . from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean, col, split, regexp_extract, when, lit) from pyspark.ml.feature import ( VectorAssembler, StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df.withColumnRenamed('Survived', 'label') testing_df = testing_df.withColumn('label', lit(0)) datasets_list = [training_df, testing_df] for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn( \"Initial\", regexp_extract(col(\"Name\"), \"([A-Za-z]+)\\.\", 1)) datasets_list[index] = dataset misspelled_initials = [ 'Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don' ] correct_initials = [ 'Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr' ] for index, dataset in enumerate(datasets_list): dataset = dataset.replace(misspelled_initials, correct_initials) datasets_list[index] = dataset initials_age = {\"Miss\": 22, \"Other\": 46, \"Master\": 5, \"Mr\": 33, \"Mrs\": 36} for index, dataset in enumerate(datasets_list): for initial, initial_age in initials_age.items(): dataset = dataset.withColumn( \"Age\", when((dataset[\"Initial\"] == initial) & (dataset[\"Age\"].isNull()), initial_age).otherwise( dataset[\"Age\"])) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.na.fill({\"Embarked\": 'S'}) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn(\"Family_Size\", col('SibSp')+col('Parch')) dataset = dataset.withColumn('Alone', lit(0)) dataset = dataset.withColumn( \"Alone\", when(dataset[\"Family_Size\"] == 0, 1).otherwise(dataset[\"Alone\"])) datasets_list[index] = dataset text_fields = [\"Sex\", \"Embarked\", \"Initial\"] for column in text_fields: for index, dataset in enumerate(datasets_list): dataset = StringIndexer( inputCol=column, outputCol=column+\"_index\").\\ fit(dataset).\\ transform(dataset) datasets_list[index] = dataset non_required_columns = [\"Name\", \"Embarked\", \"Sex\", \"Initial\"] for index, dataset in enumerate(datasets_list): dataset = dataset.drop(*non_required_columns) datasets_list[index] = dataset training_df = datasets_list[TRAINING_DF_INDEX] testing_df = datasets_list[TESTING_DF_INDEX] assembler = VectorAssembler( inputCols=training_df.columns[:], outputCol=\"features\") assembler.setHandleInvalid('skip') features_training = assembler.transform(training_df) (features_training, features_evaluation) =\\ features_training.randomSplit([0.8, 0.2], seed=33) features_testing = assembler.transform(testing_df)","title":"preprocessor_code Example"},{"location":"pca-api/","text":"PCA API PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn (used in this microservice), PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components. PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter whiten = True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm, more information about this algorithm in scikit-learn PCA docs . Create an image plot POST CLUSTER_IP:5006/images/<parent_filename> The request uses a parent_filename as a dataset filename, the body contains the json fields: { \"pca_filename\": \"image_plot_filename\", \"label_name\": \"dataset_label_column\" } The label_name is the label name column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in JSON: { \"pca_filename\": \"image_plot_filename\", \"label_name\": null } Delete an image plot DELETE CLUSTER_IP:5006/images/<image_plot_filename> Deletes an image plot from the database. Read the filenames of the created images GET CLUSTER_IP:5006/images Returns a list with all created image plot filenames. Read an image plot GET CLUSTER_IP:5006/images/<image_plot_filename> Returns the image plot of filename specified. Images plot examples This examples use the titanic challengue datasets . Titanic Train dataset Titanic Test dataset","title":"PCA API"},{"location":"pca-api/#pca-api","text":"PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn (used in this microservice), PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components. PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter whiten = True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm, more information about this algorithm in scikit-learn PCA docs .","title":"PCA API"},{"location":"pca-api/#create-an-image-plot","text":"POST CLUSTER_IP:5006/images/<parent_filename> The request uses a parent_filename as a dataset filename, the body contains the json fields: { \"pca_filename\": \"image_plot_filename\", \"label_name\": \"dataset_label_column\" } The label_name is the label name column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in JSON: { \"pca_filename\": \"image_plot_filename\", \"label_name\": null }","title":"Create an image plot"},{"location":"pca-api/#delete-an-image-plot","text":"DELETE CLUSTER_IP:5006/images/<image_plot_filename> Deletes an image plot from the database.","title":"Delete an image plot"},{"location":"pca-api/#read-the-filenames-of-the-created-images","text":"GET CLUSTER_IP:5006/images Returns a list with all created image plot filenames.","title":"Read the filenames of the created images"},{"location":"pca-api/#read-an-image-plot","text":"GET CLUSTER_IP:5006/images/<image_plot_filename> Returns the image plot of filename specified.","title":"Read an image plot"},{"location":"pca-api/#images-plot-examples","text":"This examples use the titanic challengue datasets .","title":"Images plot examples"},{"location":"pca-api/#titanic-train-dataset","text":"","title":"Titanic Train dataset"},{"location":"pca-api/#titanic-test-dataset","text":"","title":"Titanic Test dataset"},{"location":"projection-api/","text":"Projection API Projection API microservice provides an API to make a projection from file inserted in database service, generating a new file and storing in database. Create projection from an inserted file POST CLUSTER_IP:5001/projections/<filename> Post request where filename is the name of the file to create a projection for. { \"projection_filename\" : \"filename_to_save_projection\", \"fields\" : [\"list\", \"of\", \"fields\"] }","title":"Projection API"},{"location":"projection-api/#projection-api","text":"Projection API microservice provides an API to make a projection from file inserted in database service, generating a new file and storing in database.","title":"Projection API"},{"location":"projection-api/#create-projection-from-an-inserted-file","text":"POST CLUSTER_IP:5001/projections/<filename> Post request where filename is the name of the file to create a projection for. { \"projection_filename\" : \"filename_to_save_projection\", \"fields\" : [\"list\", \"of\", \"fields\"] }","title":"Create projection from an inserted file"},{"location":"python-apis/","text":"python-client APIs Database API read_resume_files read_resume_files(pretty_response=True) pretty_response : returns indented string for visualization(default: True , returns dict if False ) (default True , if False , return dict) read_file read_file(filename, skip=0, limit=10, query={}, pretty_response=True) filename : name of file skip : number of rows to skip in pagination(default: 0 ) limit : number of rows to return in pagination(default: 10 ) (maximum is set at 20 rows per request) query : query to make in MongoDB(default: empty query ) pretty_response : returns indented string for visualization(default: True , returns dict if False ) create_file create_file(filename, url, pretty_response=True) filename : name of file to be created url : url to CSV file pretty_response : returns indented string for visualization (default: True , returns dict if False ) delete_file delete_file(filename, pretty_response=True) filename : name of the file to be deleted pretty_response : returns indented string for visualization (default: True , returns dict if False ) Projection API create_projection create_projection(filename, projection_filename, fields, pretty_response=True) filename : name of the file to make projection projection_filename : name of file used to create projection fields : list with fields to make projection pretty_response : returns indented string for visualization (default: True , returns dict if False ) Data type handler API change_file_type change_file_type(filename, fields_dict, pretty_response=True) filename : name of file fields_dict : dictionary with field : number or field : string keys pretty_response : returns indented string for visualization (default: True , returns dict if False ) Histogram API create_histogram create_histogram(filename, histogram_filename, fields, pretty_response=True) filename : name of file to make histogram histogram_filename : name of file used to create histogram fields : list with fields to make histogram pretty_response : returns indented string for visualization (default: True , returns dict if False ) t-SNE API create_image_plot create_image_plot(tsne_filename, parent_filename, label_name=None, pretty_response=True) parent_filename : name of file to make histogram tsne_filename : name of file used to create image plot label_name : label name to dataset with labeled tuples (default: None , to datasets without labeled tuples) pretty_response : returns indented string for visualization (default: True , returns dict if False ) read_image_plot_filenames read_image_plot_filenames(pretty_response=True) pretty_response : returns indented string for visualization (default: True , returns dict if False ) read_image_plot read_image_plot(tsne_filename, pretty_response=True) tsne_filename: filename of a created image plot pretty_response : returns indented string for visualization (default: True , returns dict if False ) delete_image_plot delete_image_plot(tsne_filename, pretty_response=True) tsne_filename : filename of a created image plot pretty_response : returns indented string for visualization (default: True , returns dict if False ) PCA API create_image_plot create_image_plot(tsne_filename, parent_filename, label_name=None, pretty_response=True) parent_filename : name of file to make histogram pca_filename : filename used to create image plot label_name : label name to dataset with labeled tuples (default: None , to datasets without labeled tuples) pretty_response : returns indented string for visualization (default: True , returns dict if False ) read_image_plot_filenames read_image_plot_filenames(pretty_response=True) pretty_response : returns indented string for visualization (default: True , returns dict if False ) read_image_plot read_image_plot(pca_filename, pretty_response=True) pca_filename : filename of a created image plot pretty_response : returns indented string for visualization (default: True , returns dict if False ) delete_image_plot delete_image_plot(pca_filename, pretty_response=True) pca_filename : filename of a created image plot pretty_response : returns indented string for visualization (default: True , returns dict if False ) Model builder API create_model create_model(training_filename, test_filename, preprocessor_code, model_classificator, pretty_response=True) training_filename : name of file to be used in training test_filename : name of file to be used in test preprocessor_code : Python3 code for pyspark preprocessing model model_classificator : list of initial classificators to be used in model pretty_response : returns indented string for visualization (default: True , returns dict if False ) model_classificator lr : LogisticRegression dt : DecisionTreeClassifier rf : RandomForestClassifier gb : Gradient-boosted tree classifier nb : NaiveBayes to send a request with LogisticRegression and NaiveBayes Classifiers: create_model(training_filename, test_filename, preprocessor_code, [\"lr\", \"nb\"]) preprocessor_code environment The Python 3 preprocessing code must use the environment instances as below: training_df (Instantiated): Spark Dataframe instance training filename testing_df (Instantiated): Spark Dataframe instance testing filename The preprocessing code must instantiate the variables as below, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for training the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluating trained model features_testing (Not Instantiated): Spark Dataframe instance for testing the model In case you don't want to evaluate the model, set features_evaluation as None . Handy methods self.fields_from_dataframe(dataframe, is_string) This method returns string or number fields as a string list from a DataFrame. dataframe : DataFrame instance is_string : Boolean parameter(if True , the method returns the string DataFrame fields, otherwise, returns the numbers DataFrame fields)","title":"python-client APIs"},{"location":"python-apis/#python-client-apis","text":"","title":"python-client APIs"},{"location":"python-apis/#database-api","text":"","title":"Database API"},{"location":"python-apis/#read_resume_files","text":"read_resume_files(pretty_response=True) pretty_response : returns indented string for visualization(default: True , returns dict if False ) (default True , if False , return dict)","title":"read_resume_files"},{"location":"python-apis/#read_file","text":"read_file(filename, skip=0, limit=10, query={}, pretty_response=True) filename : name of file skip : number of rows to skip in pagination(default: 0 ) limit : number of rows to return in pagination(default: 10 ) (maximum is set at 20 rows per request) query : query to make in MongoDB(default: empty query ) pretty_response : returns indented string for visualization(default: True , returns dict if False )","title":"read_file"},{"location":"python-apis/#create_file","text":"create_file(filename, url, pretty_response=True) filename : name of file to be created url : url to CSV file pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"create_file"},{"location":"python-apis/#delete_file","text":"delete_file(filename, pretty_response=True) filename : name of the file to be deleted pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"delete_file"},{"location":"python-apis/#projection-api","text":"","title":"Projection API"},{"location":"python-apis/#create_projection","text":"create_projection(filename, projection_filename, fields, pretty_response=True) filename : name of the file to make projection projection_filename : name of file used to create projection fields : list with fields to make projection pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"create_projection"},{"location":"python-apis/#data-type-handler-api","text":"","title":"Data type handler API"},{"location":"python-apis/#change_file_type","text":"change_file_type(filename, fields_dict, pretty_response=True) filename : name of file fields_dict : dictionary with field : number or field : string keys pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"change_file_type"},{"location":"python-apis/#histogram-api","text":"","title":"Histogram API"},{"location":"python-apis/#create_histogram","text":"create_histogram(filename, histogram_filename, fields, pretty_response=True) filename : name of file to make histogram histogram_filename : name of file used to create histogram fields : list with fields to make histogram pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"create_histogram"},{"location":"python-apis/#t-sne-api","text":"","title":"t-SNE API"},{"location":"python-apis/#create_image_plot","text":"create_image_plot(tsne_filename, parent_filename, label_name=None, pretty_response=True) parent_filename : name of file to make histogram tsne_filename : name of file used to create image plot label_name : label name to dataset with labeled tuples (default: None , to datasets without labeled tuples) pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"create_image_plot"},{"location":"python-apis/#read_image_plot_filenames","text":"read_image_plot_filenames(pretty_response=True) pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"read_image_plot_filenames"},{"location":"python-apis/#read_image_plot","text":"read_image_plot(tsne_filename, pretty_response=True) tsne_filename: filename of a created image plot pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"read_image_plot"},{"location":"python-apis/#delete_image_plot","text":"delete_image_plot(tsne_filename, pretty_response=True) tsne_filename : filename of a created image plot pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"delete_image_plot"},{"location":"python-apis/#pca-api","text":"","title":"PCA API"},{"location":"python-apis/#create_image_plot_1","text":"create_image_plot(tsne_filename, parent_filename, label_name=None, pretty_response=True) parent_filename : name of file to make histogram pca_filename : filename used to create image plot label_name : label name to dataset with labeled tuples (default: None , to datasets without labeled tuples) pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"create_image_plot"},{"location":"python-apis/#read_image_plot_filenames_1","text":"read_image_plot_filenames(pretty_response=True) pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"read_image_plot_filenames"},{"location":"python-apis/#read_image_plot_1","text":"read_image_plot(pca_filename, pretty_response=True) pca_filename : filename of a created image plot pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"read_image_plot"},{"location":"python-apis/#delete_image_plot_1","text":"delete_image_plot(pca_filename, pretty_response=True) pca_filename : filename of a created image plot pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"delete_image_plot"},{"location":"python-apis/#model-builder-api","text":"","title":"Model builder API"},{"location":"python-apis/#create_model","text":"create_model(training_filename, test_filename, preprocessor_code, model_classificator, pretty_response=True) training_filename : name of file to be used in training test_filename : name of file to be used in test preprocessor_code : Python3 code for pyspark preprocessing model model_classificator : list of initial classificators to be used in model pretty_response : returns indented string for visualization (default: True , returns dict if False )","title":"create_model"},{"location":"python-apis/#model_classificator","text":"lr : LogisticRegression dt : DecisionTreeClassifier rf : RandomForestClassifier gb : Gradient-boosted tree classifier nb : NaiveBayes to send a request with LogisticRegression and NaiveBayes Classifiers: create_model(training_filename, test_filename, preprocessor_code, [\"lr\", \"nb\"])","title":"model_classificator"},{"location":"python-apis/#preprocessor_code-environment","text":"The Python 3 preprocessing code must use the environment instances as below: training_df (Instantiated): Spark Dataframe instance training filename testing_df (Instantiated): Spark Dataframe instance testing filename The preprocessing code must instantiate the variables as below, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for training the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluating trained model features_testing (Not Instantiated): Spark Dataframe instance for testing the model In case you don't want to evaluate the model, set features_evaluation as None .","title":"preprocessor_code environment"},{"location":"python-apis/#handy-methods","text":"self.fields_from_dataframe(dataframe, is_string) This method returns string or number fields as a string list from a DataFrame. dataframe : DataFrame instance is_string : Boolean parameter(if True , the method returns the string DataFrame fields, otherwise, returns the numbers DataFrame fields)","title":"Handy methods"},{"location":"t-sne-api/","text":"t-SNE API The T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability, more information about this algorithm in its Wiki page . Create an image plot POST CLUSTER_IP:5005/images/<parent_filename> The request uses a parent_filename as a dataset inserted filename, the body contains the JSON fields: { \"tsne_filename\": \"image_plot_filename\", \"label_name\": \"dataset_label_column\" } The label_name is the label name of the column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"tsne_filename\": \"image_plot_filename\", \"label_name\": null } Delete an image plot DELETE CLUSTER_IP:5005/images/<image_plot_filename> Deletes an image plot by specifying its file name. Read the filenames of the created images GET CLUSTER_IP:5005/images Returns a list with all created images plot file name. Read an image plot GET CLUSTER_IP:5005/images/<image_plot_filename> Returns the image plot of the specified file name. Image plot examples These examples use the titanic challengue datasets . Titanic Train dataset Titanic Test dataset","title":"t-SNE API"},{"location":"t-sne-api/#t-sne-api","text":"The T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability, more information about this algorithm in its Wiki page .","title":"t-SNE API"},{"location":"t-sne-api/#create-an-image-plot","text":"POST CLUSTER_IP:5005/images/<parent_filename> The request uses a parent_filename as a dataset inserted filename, the body contains the JSON fields: { \"tsne_filename\": \"image_plot_filename\", \"label_name\": \"dataset_label_column\" } The label_name is the label name of the column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"tsne_filename\": \"image_plot_filename\", \"label_name\": null }","title":"Create an image plot"},{"location":"t-sne-api/#delete-an-image-plot","text":"DELETE CLUSTER_IP:5005/images/<image_plot_filename> Deletes an image plot by specifying its file name.","title":"Delete an image plot"},{"location":"t-sne-api/#read-the-filenames-of-the-created-images","text":"GET CLUSTER_IP:5005/images Returns a list with all created images plot file name.","title":"Read the filenames of the created images"},{"location":"t-sne-api/#read-an-image-plot","text":"GET CLUSTER_IP:5005/images/<image_plot_filename> Returns the image plot of the specified file name.","title":"Read an image plot"},{"location":"t-sne-api/#image-plot-examples","text":"These examples use the titanic challengue datasets .","title":"Image plot examples"},{"location":"t-sne-api/#titanic-train-dataset","text":"","title":"Titanic Train dataset"},{"location":"t-sne-api/#titanic-test-dataset","text":"","title":"Titanic Test dataset"},{"location":"usage/","text":"Usage learningOrchestra can be used with the Microservices REST API or with the learning-orchestra-client Python package . Microservices REST APIs Database API - Download and handle datasets in a database. Projection API - Make projections of stored datasets using Spark cluster. Data type API - Change dataset fields type between number and text. Histogram API - Make histograms of stored datasets. t-SNE API - Make a t-SNE image plot of stored datasets. PCA API - Make a PCA image plot of stored datasets. Model builder API - Create a prediction model from pre-processed datasets using Spark cluster. Spark Microservices The Projection, t-SNE, PCA and Model builder microservices uses the Spark microservice to work. By default, this microservice has only one instance. In case your data processing requires more computing power, you can scale this microservice. To do this, with learningOrchestra already deployed, run the following in the manager machine of your Docker swarm cluster: docker service scale microservice_sparkworker=NUMBER_OF_INSTANCES * NUMBER_OF_INSTANCES is the number of Spark microservice instances which you require. Choose it according to your cluster resources and your resource requirements. Database GUI NoSQLBooster- MongoDB GUI performs several database tasks such as file visualization, queries, projections and file extraction to CSV and JSON formats. It can be util to accomplish some these tasks with your processed dataset or get your prediction results. Read the Database API docs for more info on configuring this tool.","title":"Usage"},{"location":"usage/#usage","text":"learningOrchestra can be used with the Microservices REST API or with the learning-orchestra-client Python package .","title":"Usage"},{"location":"usage/#microservices-rest-apis","text":"Database API - Download and handle datasets in a database. Projection API - Make projections of stored datasets using Spark cluster. Data type API - Change dataset fields type between number and text. Histogram API - Make histograms of stored datasets. t-SNE API - Make a t-SNE image plot of stored datasets. PCA API - Make a PCA image plot of stored datasets. Model builder API - Create a prediction model from pre-processed datasets using Spark cluster.","title":"Microservices REST APIs"},{"location":"usage/#spark-microservices","text":"The Projection, t-SNE, PCA and Model builder microservices uses the Spark microservice to work. By default, this microservice has only one instance. In case your data processing requires more computing power, you can scale this microservice. To do this, with learningOrchestra already deployed, run the following in the manager machine of your Docker swarm cluster: docker service scale microservice_sparkworker=NUMBER_OF_INSTANCES * NUMBER_OF_INSTANCES is the number of Spark microservice instances which you require. Choose it according to your cluster resources and your resource requirements.","title":"Spark Microservices"},{"location":"usage/#database-gui","text":"NoSQLBooster- MongoDB GUI performs several database tasks such as file visualization, queries, projections and file extraction to CSV and JSON formats. It can be util to accomplish some these tasks with your processed dataset or get your prediction results. Read the Database API docs for more info on configuring this tool.","title":"Database GUI"}]}